{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting Unidecode\n",
      "  Obtaining dependency information for Unidecode from https://files.pythonhosted.org/packages/84/b7/6ec57841fb67c98f52fc8e4a2d96df60059637cba077edc569a302a8ffc7/Unidecode-1.3.8-py3-none-any.whl.metadata\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Unidecode\n",
      "Successfully installed Unidecode-1.3.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/daoan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Train Loss: 1.0355 | Val Loss: 0.9417, Val Acc: 0.5873\n",
      "Epoch [2/10] Train Loss: 0.9321 | Val Loss: 0.9269, Val Acc: 0.5873\n",
      "Epoch [3/10] Train Loss: 0.9263 | Val Loss: 0.9250, Val Acc: 0.5873\n",
      "Epoch [4/10] Train Loss: 0.9240 | Val Loss: 0.9245, Val Acc: 0.5873\n",
      "Epoch [5/10] Train Loss: 0.9290 | Val Loss: 0.9247, Val Acc: 0.5873\n",
      "Epoch [6/10] Train Loss: 0.9226 | Val Loss: 0.9247, Val Acc: 0.5873\n",
      "Epoch [7/10] Train Loss: 0.9234 | Val Loss: 0.9251, Val Acc: 0.5873\n",
      "Epoch [8/10] Train Loss: 0.9236 | Val Loss: 0.9249, Val Acc: 0.5873\n",
      "Epoch [9/10] Train Loss: 0.9269 | Val Loss: 0.9245, Val Acc: 0.5873\n",
      "Epoch [10/10] Train Loss: 0.9258 | Val Loss: 0.9253, Val Acc: 0.5873\n",
      "Training finished!\n",
      "Test Loss = 0.9511 | Test Accuracy = 0.5708\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Thư viện xử lý văn bản\n",
    "import nltk\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Thư viện chia train/val/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Thiết lập random seed\n",
    "# -----------------------------\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Đảm bảo đã cài đặt nltk stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Đọc dữ liệu\n",
    "# -----------------------------\n",
    "\"\"\"\n",
    "Giả sử file all-data.csv có cấu trúc hai cột:\n",
    "- sentiment: 'positive', 'negative' hoặc 'neutral'\n",
    "- content: nội dung văn bản tin tức\n",
    "Ví dụ:\n",
    "sentiment,content\n",
    "neutral,\"According to Gran, the company...\"\n",
    "negative,\"The company laid off tens of employees...\"\n",
    "positive,\"This move would increase capacity...\"\n",
    "\"\"\"\n",
    "\n",
    "dataset_path = \"all-data.csv\"  # chỉnh lại đường dẫn file nếu cần\n",
    "headers = [\"sentiment\", \"content\"]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    dataset_path,\n",
    "    names=headers,\n",
    "    encoding=\"ISO-8859-1\"\n",
    ")\n",
    "\n",
    "# Bỏ các dòng trống hoặc NaN (nếu có)\n",
    "df.dropna(subset=[\"sentiment\", \"content\"], inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Gán label dạng chuỗi -> ID (0/1/2)\n",
    "# -----------------------------\n",
    "unique_sentiments = df[\"sentiment\"].unique().tolist()  # ví dụ: ['neutral', 'negative', 'positive']\n",
    "classes = {class_name: idx for idx, class_name in enumerate(unique_sentiments)}\n",
    "df[\"sentiment\"] = df[\"sentiment\"].apply(lambda x: classes[x])\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Tiền xử lý dữ liệu văn bản\n",
    "# -----------------------------\n",
    "# - lower casing\n",
    "# - bỏ dấu unicode\n",
    "# - bỏ stopwords\n",
    "# - stemming\n",
    "# - xóa dấu câu (punctuation)\n",
    "# -----------------------------\n",
    "english_stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def text_normalize(text):\n",
    "    # 1) lower\n",
    "    text = text.lower()\n",
    "    # 2) unidecode\n",
    "    text = unidecode.unidecode(text)\n",
    "    # 3) strip\n",
    "    text = text.strip()\n",
    "    # 4) remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # 5) remove stopwords\n",
    "    words = [w for w in text.split() if w not in english_stop_words]\n",
    "    # 6) stemming\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"content\"] = df[\"content\"].apply(text_normalize)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Tạo vocabulary\n",
    "# -----------------------------\n",
    "vocab = set()\n",
    "for sentence in df[\"content\"].tolist():\n",
    "    for token in sentence.split():\n",
    "        vocab.add(token)\n",
    "\n",
    "# Thêm token đặc biệt\n",
    "vocab = list(vocab)\n",
    "vocab.append(\"UNK\")\n",
    "vocab.append(\"PAD\")\n",
    "\n",
    "# Tạo word_to_idx\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Hàm chuyển văn bản -> chuỗi ID\n",
    "# -----------------------------\n",
    "def transform(text, word_to_idx, max_seq_len=32):\n",
    "    tokens = []\n",
    "    for w in text.split():\n",
    "        if w in word_to_idx:\n",
    "            tokens.append(word_to_idx[w])\n",
    "        else:\n",
    "            tokens.append(word_to_idx[\"UNK\"])\n",
    "\n",
    "    # padding / cắt chuỗi\n",
    "    if len(tokens) < max_seq_len:\n",
    "        tokens += [word_to_idx[\"PAD\"]] * (max_seq_len - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_seq_len]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Chia dữ liệu train/val/test\n",
    "#    tỉ lệ ví dụ: train 70%, val 20%, test 10%\n",
    "# -----------------------------\n",
    "texts = df[\"content\"].tolist()\n",
    "labels = df[\"sentiment\"].tolist()\n",
    "\n",
    "# 70% train, còn lại 30% -> chia tiếp val/test (2/3 val, 1/3 test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts, labels,\n",
    "    test_size=0.30,\n",
    "    random_state=seed,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Trong 30% còn lại, ta chia 2/3 val, 1/3 test => val=0.2, test=0.1 (tương đối)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.33,  # khoảng ~10% của tổng\n",
    "    random_state=seed,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Tạo Dataset và DataLoader\n",
    "# -----------------------------\n",
    "class FinancialNews(Dataset):\n",
    "    def __init__(self, X, y, word_to_idx, max_seq_len=32, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            text = self.transform(text, self.word_to_idx, self.max_seq_len)\n",
    "        text_tensor = torch.tensor(text, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return text_tensor, label_tensor\n",
    "\n",
    "max_seq_len = 32\n",
    "\n",
    "train_dataset = FinancialNews(\n",
    "    X_train, y_train,\n",
    "    word_to_idx=word_to_idx,\n",
    "    max_seq_len=max_seq_len,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = FinancialNews(\n",
    "    X_val, y_val,\n",
    "    word_to_idx=word_to_idx,\n",
    "    max_seq_len=max_seq_len,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = FinancialNews(\n",
    "    X_test, y_test,\n",
    "    word_to_idx=word_to_idx,\n",
    "    max_seq_len=max_seq_len,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Định nghĩa mô hình RNN\n",
    "# -----------------------------\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, n_classes, dropout_prob=0.2):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Ở đây dùng RNN thường. Nếu muốn LSTM thì thay thành nn.LSTM(...)\n",
    "        # hoặc BiLSTM: nn.LSTM(..., bidirectional=True)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc1 = nn.Linear(hidden_size, 16)\n",
    "        self.fc2 = nn.Linear(16, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        x = self.embedding(x)     # -> (batch_size, seq_len, embedding_dim)\n",
    "        rnn_out, h_n = self.rnn(x)  # -> (batch_size, seq_len, hidden_size)\n",
    "        # Lấy hidden state tại time step cuối\n",
    "        x = rnn_out[:, -1, :]     # -> (batch_size, hidden_size)\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Tạo instance model\n",
    "# -----------------------------\n",
    "n_classes = len(unique_sentiments)  # số nhãn (3: pos, neg, neutral)\n",
    "embedding_dim = 64\n",
    "hidden_size = 64\n",
    "n_layers = 2\n",
    "dropout_prob = 0.2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SentimentClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    n_layers=n_layers,\n",
    "    n_classes=n_classes,\n",
    "    dropout_prob=dropout_prob\n",
    ").to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 11. Loss, optimizer\n",
    "# -----------------------------\n",
    "lr = 1e-4\n",
    "epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# -----------------------------\n",
    "# 12. Định nghĩa hàm train + evaluate\n",
    "# -----------------------------\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def fit(model, train_loader, val_loader, criterion, optimizer, device, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Tính loss trên tập train & val\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "# -----------------------------\n",
    "# 13. Huấn luyện mô hình\n",
    "# -----------------------------\n",
    "fit(model, train_loader, val_loader, criterion, optimizer, device, epochs=epochs)\n",
    "\n",
    "# -----------------------------\n",
    "# 14. Đánh giá mô hình trên tập test\n",
    "# -----------------------------\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss = {test_loss:.4f} | Test Accuracy = {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model to sentiment_classifier.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daoan/Projects/AIO-2024/.venv/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:4662: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with RNN_TANH can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Giả sử bạn đã có sẵn word_to_idx, vocab_size, max_seq_len và model \n",
    "# đã được huấn luyện hoặc sẵn sàng để export.\n",
    "# Dưới đây minh hoạ ngắn gọn một mô hình tương tự SentimentClassifier\n",
    "# rồi export sang ONNX.\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embedding_dim=64,\n",
    "        hidden_size=64, \n",
    "        n_layers=2, \n",
    "        n_classes=3,     # ví dụ 3 class: positive/negative/neutral\n",
    "        dropout_prob=0.2\n",
    "    ):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc1 = nn.Linear(hidden_size, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x.shape = (batch_size, seq_len)\n",
    "        Mỗi phần tử trong x là chỉ mục (index) của từ trong vocab\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)   # => (batch_size, seq_len, embedding_dim)\n",
    "        # RNN trả về (batch_size, seq_len, hidden_size)\n",
    "        x, hn = self.rnn(x)\n",
    "        # Ta lấy hidden state ở time step cuối:\n",
    "        # x[:, -1, :] => (batch_size, hidden_size)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ------------------------------\n",
    "    # 1. Khởi tạo mô hình (đã huấn luyện hoặc load checkpoint)\n",
    "    # ------------------------------\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Giả sử vocab_size = 5000, số class = 3\n",
    "    vocab_size = 5000\n",
    "    n_classes = 3\n",
    "    \n",
    "    model = SentimentClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=64,\n",
    "        hidden_size=64,\n",
    "        n_layers=2,\n",
    "        n_classes=n_classes,\n",
    "        dropout_prob=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Ở đây giả sử mô hình đã load trọng số (nếu có), ví dụ:\n",
    "    # model.load_state_dict(torch.load(\"sentiment_model.pth\", map_location=device))\n",
    "    # model.eval()\n",
    "    \n",
    "    # ------------------------------\n",
    "    # 2. Tạo dummy input để export\n",
    "    # ------------------------------\n",
    "    max_seq_len = 32  # độ dài cố định mỗi batch\n",
    "    batch_size = 1     # xuất mô hình cho batch_size=1 (thường để dynamic_axes)\n",
    "    \n",
    "    # Tạo random đầu vào (mô phỏng tokens ID)\n",
    "    # Mỗi token ID nằm trong [0, vocab_size)\n",
    "    dummy_input = torch.randint(\n",
    "        low=0,\n",
    "        high=vocab_size,  # exclusive\n",
    "        size=(batch_size, max_seq_len),\n",
    "        dtype=torch.long\n",
    "    ).to(device)\n",
    "    \n",
    "    # ------------------------------\n",
    "    # 3. Export sang ONNX\n",
    "    # ------------------------------\n",
    "    # Filename ONNX\n",
    "    onnx_filename = \"sentiment_classifier.onnx\"\n",
    "    \n",
    "    # Đặt chế độ eval\n",
    "    model.eval()\n",
    "    \n",
    "    # `dynamic_axes` giúp mô hình linh hoạt kích thước batch hoặc seq_len\n",
    "    #  - \"input\": {0: \"batch_size\", 1: \"seq_len\"}\n",
    "    #  - \"output\": {0: \"batch_size\"}\n",
    "    # opset_version >= 11 để compatible nhiều công cụ\n",
    "    torch.onnx.export(\n",
    "        model,              # mô hình\n",
    "        dummy_input,        # data giả\n",
    "        onnx_filename,      # tên file .onnx\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "            \"output\": {0: \"batch_size\"}\n",
    "        },\n",
    "        opset_version=11\n",
    "    )\n",
    "    \n",
    "    print(f\"Exported model to {onnx_filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
